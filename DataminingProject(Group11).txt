install.packages("Hmisc")
library(Hmisc)

data=read.csv("D:DataMiningAss/project/iris.data",header=T, na.strings ="")
fix(data)
cleandata <- data[complete.cases(data),]

##Insights into Iris Dataset

##Renaming column name for Iris dataset
library("plyr")
dataset = rename(cleandata, c("X5.1"="sepal_len", "X3.5"="sepal_wid", "X1.4"="petal_len", "X0.2"="petal_wid", "Iris.setosa"="class"  ))
fix(dataset)

##Boxplot for Sepal Length
boxplot(dataset$sepal_len ~ dataset$class, xlab = "Class", ylab = "Sepal length", main = "Sepal length vs Class", col=
c("red","blue","yellow"), border= "black" )

##Boxplot for Sepal Width
boxplot(dataset$sepal_wid ~ dataset$class, xlab = "Class", ylab = "Sepal width", main = "Sepal width vs Class", col=
c("red","blue","yellow"), border= "black" )

##Boxplot for Petal Length
boxplot(dataset$petal_len ~ dataset$class, xlab = "Class", ylab = "Petal length", main = "Petal length vs Class", col=
c("red","blue","yellow"), border= "black" )

##Boxplot for Petal Width
boxplot(dataset$petal_wid ~ dataset$class, xlab = "Class", ylab = "Petal width", main = "Petal width vs Class", col=
c("red","blue","yellow"), border= "black" )

## Scatter plot for Sepal length and width
plot(dataset$sepal_len , dataset$petal_wid , main = "Sepal's of Iris Data",
xlab = "Sepal Length", ylab = "Sepal Width")

## fill colour into sepal length and width scatter plot
mycolor = c("red","green3","blue")[as.factor(dataset$class)]

## Scatter plot for Sepal length and width
plot(dataset$sepal_len, dataset$sepal_wid , pch = 8, col = mycolor,
main = "Sepal's of Iris Data", xlab = "Sepal Length", ylab = "Sepal Width",
xlim = c(3,9), ylim= c(2,5))
legend('topright', legend = unique(dataset$class), col = c("red","green3","blue"),
pch = 8, bty = 'n')

## Scatter plot for Petal length and width
plot(dataset$petal_len, dataset$petal_wid , pch = 8, col = mycolor,
main = "Petal's of Iris Data", xlab = "Petal Length", ylab = "Petal Width",
xlim = c(0,10), ylim= c(0,3))
legend('topright', legend = unique(dataset$class), col = c("red","green3","blue"),
pch = 8, bty = 'n')



library(tree)
attach(dataset)
set.seed(123)
datatrain=sample(1:nrow(dataset),115)
train=dataset[datatrain,]
test=dataset[-datatrain,]

library(e1071)
## DECISION TREE :

#To make a decision tree that classify the Target attribute based on all other attributes
tree.mydata=tree(class~.,dataset)
summary(tree.mydata)

plot(tree.mydata)
text(tree.mydata,pretty=0)

tree.mydata

#The predict() function is used to test the model. The argument 
#type="class" instructs R to return the actual class prediction
tree.pred=predict(tree.mydata,type="class")

#Create the confusion matrix
table(tree.pred,class)

#Correct prediction rate
mean(tree.pred==class)

#Error prediction rate
mean(tree.pred!=class)

#Random select a sample of 115 observations of the data set as a training set and the rest 
#of the data set as a test set.

set.seed(123)
train=sample(1:nrow(dataset), 115)
mydata.test=dataset[-train,]
class.test=class[-train]
tree.mydata=tree(class~.,dataset,subset=train)
tree.pred=predict(tree.mydata,mydata.test,type="class")
table(tree.pred,class.test)
mean(tree.pred!=class.test)

#Bagging and Random Forests

install.packages('randomForest')
library(randomForest)

#Some arguments of function randomForest():
#mtry: the number of variables randomly sampled as candidates at each split.
#ntree: the number of trees to grow.

#ntree=500 indicates that 500 trees are generated by bagging
#mtry=10 indicates that all 10 variables are used at each split.

set.seed(123)
tree.mydata=randomForest(class~.,dataset,subset=train, ntree=500,mtry=4)
tree.pred=predict(tree.mydata,mydata.test,type="class")
table(tree.pred,class.test)
mean(tree.pred!=class.test)

#Check with 1500 trees
set.seed(123)
tree.mydata=randomForest(class~.,dataset,subset=train, ntree=1500,mtry=4)
tree.pred=predict(tree.mydata,mydata.test,type="class")
table(tree.pred,class.test)
mean(tree.pred!=class.test)


#By default, randomForest() uses about sqrt(p) variables when building a random forest of 
classification trees. sqrt(4)=2.

set.seed(123)
tree.mydata=randomForest(class~.,dataset,subset=train, ntree=500,mtry=2)
tree.pred=predict(tree.mydata,mydata.test,type="class")
table(tree.pred,class.test)
mean(tree.pred!=class.test)

set.seed(123)
tree.mydata=randomForest(class~.,dataset,subset=train, ntree=500,mtry=3)
tree.pred=predict(tree.mydata,mydata.test,type="class")
table(tree.pred,class.test)
mean(tree.pred!=class.test)


##NAIVE BAYER : 
library(e1071)
set.seed(123)
datatrain=sample(1:nrow(dataset),115)
train=dataset[datatrain,]
test=dataset[-datatrain,]

test.label = class[-datatrain] 
NB_model = naiveBayes(class~. , data = train)
NB_Prediction = predict( NB_model , test)
table(NB_Prediction, test.label)
mean(NB_Prediction!=test.label)

##for train data
train.pred = predict(NB_model, train)
table(train$class, train.pred)

train.label = class[datatrain] 
mean( train.pred != train.label ) 


##SUPPORT VECTOR MACHINE :

set.seed(123)
datatrain=sample(1:nrow(dataset),115)
train=dataset[datatrain,]
test=dataset[-datatrain,]

cost=10^(-3:5)
gamma=(1:5)

tune.out=tune(svm,class~.,data=train,kernel="linear",ranges=list(gamma=gamma,cost=cost))
tune.out$best.model
summary(tune.out)
summary(tune.out$best.model)

##Linear kernel
set.seed(123)
svm.linear=svm(class~.,data=train, kernel="linear",gamma=tune.out$best.parameter$gamma, cost=tune.out$best.parameter$cost)
summary(svm.linear)


##train
train.pred = predict(svm.linear,train)
train.table = table(Predict = train.pred,Truth = train$class)
print(train.table)
mean(train.pred!=train$class)


##test
test.pred = predict(svm.linear,test)
test.table = table(Predict = test.pred,Truth = test$class)
print(test.table)
mean(test.pred!=test$class)

##radial
set.seed(123)
svm.radial=svm(class~.,data=train, kernel="radial",gamma=tune.out$best.parameter$gamma, cost=tune.out$best.parameter$cost)
summary(svm.radial)

##train
train.pred = predict(svm.radial,train)
train.table = table(Predict = train.pred,Truth = train$class)
print(train.table)
mean(train.pred!=train$class)

##test
test.pred = predict(svm.radial,test)
test.table = table(Predict = test.pred, Truth = test$class)
print(test.table)
mean(test.pred!=test$class)

##poly 
set.seed(123)
degree=(1:10)
svm.poly = svm(class~., data =train, kernel = "polynomial",ranges=list(degree=degree),gamma=tune.out$best.parameter$gamma,cost=tune.out$best.parameter$cost)
summary(svm.poly)

##train
train.pred = predict(svm.poly,train)
train.table = table(Predict = train.pred,Truth = train$class)
print(train.table)
mean(train.pred!=train$class)

##test
test.pred = predict(svm.poly,test)
test.table = table(Predict = test.pred, Truth = test$class)
print(test.table)
mean(test.pred!=test$class)



